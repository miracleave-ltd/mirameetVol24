{"./":{"url":"./","title":"はじめに","keywords":"","body":"GCP初心者必見！ PythonでBigQueryの操作をしてみよう！ 第24回目の開催となるmirameetの題材はGCP初心者必見★今回のハンズオンはGoogle Cloud Storageの開発Tipsも取り込んだ内容になっています！ 事前準備 Dockerインストール GCPアカウントの作成 今回の流れ Pythonで、以下処理を行うプログラムを実行します。①CSVファイルをGCS(GoogleCloudStrage)へアップロード②アップロードしたデータをBigQueryにインサート③BigQueryにインサートしたデータを更新・削除④BigQuery内のテーブルデータをGCSへエクスポート 技術要素 公式サイトのリンクを記載しておきますので、参考にお使いください。 GCP Docker python 手順 全体手順としては次の流れで進めます。 0.事前準備内容の確認 1.GCP各種サービスの設定 2.ダウンロードしたファイルの解凍と設定 3.各処理の実行と処理結果の確認 Windows/Macの方向けに作成しております。コマンドラインツールは、個々の利用しているもので良いのですが、今回の手順は次のものを利用します。 Windows：コマンドプロンプト Mac：ターミナル "},"Section1.html":{"url":"Section1.html","title":"0. 事前準備内容の確認","keywords":"","body":"事前準備内容の確認 GCPアカウントの確認 以下URLにアクセスしGCPコンソールへログインすると下記のようなダッシュボード画面が表示されます。https://console.cloud.google.com/ Dockerの動作確認 試しに下記のコマンドを実行すると、docker-composeのバージョンが確認できます。 docker-compose version ソースファイルの取得 以下URLにアクセスし、Github画面右上の「Code」から「Download ZIP」を選択ダウンロードしたファイルは後程使用します。https://github.com/miracleave-ltd/mirameetVol24 "},"Section2.html":{"url":"Section2.html","title":"1. GCP各種サービスの設定","keywords":"","body":"GCP各種サービスの設定 この手順では、GCPサービスの設定を進めていきます。 GCSバケット作成 左上のナビゲーションメニューから、CloudStorageを選択 「バケットを作成」をクリック グローバルに一意になるように各自設定を行います。 ↓作成完了 サービスアカウントの作成 GCPの各種リソースに対して権限を作成、管理することができるようにサービスアカウントを作成します 左上のナビゲーションメニューから、IAMと管理＞サービスアカウントを選択 任意のサービスアカウント名を入力 ↓作成完了 秘密鍵ファイルの作成・ダウンロード 作成したサービスアカウントを選択 「キー」タブ＞「新しい鍵を作成」を選択 json形式のキーをダウンロード ↓ダウンロード完了 CloudStorageAPIが有効化されていることを確認 画面上部の検索窓に「Cloud Storage」と入力し、検索結果から「Cloud Storage API」を選択 ※「APIが有効です」となっていることを確認 BigQueryAPIが有効化されていることを確認 画面上部の検索窓に「BigQuery API」と入力し、検索結果から「BigQuery API」を選択 ※「APIが有効です」となっていることを確認 データセットの作成 左上のナビゲーションメニューから、BigQueryを選択 エクスプローラーの中の「▶プロジェクト名」から「データセットを作成」を選択 画面右側に「データセットを作成する」が出てくるので、「データセットID」を入力※今回は「mira_vol24」を指定 mira_vol24 ↓完了「▶プロジェクト名」の下に「▶mira_vol24」が作成される テーブルの作成 作成したデータセット「mira_vol24」に対して、クエリを実行し、テーブルを作成※今回は「mira_example」を指定 mira_example CREATE TABLE mira_vol24.mira_example ( id NUMERIC, mira_code STRING, mira_text STRING, work_date STRING ) ※データセットIDを「mira_vol24」から変更した場合は、　ご自身が指定したデータセットIDに置換して実行する必要があります ↓実行完了 先ほど実行したクエリ通りのフィールドが表示されていることを確認 このクエリは、 データセットID＝mira_vol24 テーブルID＝mira_example を指定しています ※先ほどDLしたソースファイルの\\sql\\mira_vol24.sql の中にも同じSQLがあります "},"Section3.html":{"url":"Section3.html","title":"2. ダウンロードしたファイルの解凍と設定","keywords":"","body":"ダウンロードしたファイルの解凍と設定 この手順では、GithubからダウンロードしたZipファイルを任意のディレクトリで解凍し、設定していきます。 OperationObject.pyの編集 # ------------------------------------------------------ # 変数 # ------------------------------------------------------ GOOGLE_APPLICATION_CREDENTIALS='./credential/key.json' # CSVコピー元 source_file_name = './csv/gcs-example.csv' # CSVアップロード先 url_gs_example_csv=\"gs://mira-example/gcs-example.csv\" # 手順1.1 # CSVエクスポート先 out_url_gs_example_csv=\"gs://mira-example/out-gcs-example.csv\" # 手順1.1 # GCSバケット名 bucket_name = \"mira-example\" # 手順1.2 # 操作テーブル project_id = \"erudite-pride-323410\" # 手順1.3 dataset_id = \"mira_vol24\" # 手順1.4 table_id = \"mira_example\" # 手順1.5 gsutil URI Cloud Storageのバケットの情報から「gsutil URI」をコピーして置換 （例） url_gs_example_csv=\"gs://mira-example/gcs-example.csv\" out_url_gs_example_csv=\"gs://mira-example/out-gcs-example.csv\" ↓ url_gs_example_csv=\"gs://mirameet24_test00/gcs-example.csv\" out_url_gs_example_csv=\"gs://mirameet24_test00/out-gcs-example.csv\" bucket_name グローバルに一意になるように作成したバケット名をコピーして置換 （例） bucket_name = \"mira-example\" ↓ bucket_name = \"mirameet24_test00\" project_id GCPダッシュボードの左上から「プロジェクトID」をコピーして置換 （例） project_id = \"erudite-pride-323410\" ↓ project_id = \"neural-sol-325512\" dataset_id 「データセットID」を変更した場合、任意で設定した値に置換 dataset_id = \"mira_vol24\" table_id テーブル作成時のクエリから変更した場合、任意で設定した値に置換 table_id = \"mira-example\" ダウンロードしたjson形式の秘密鍵ファイルの配置 ・ファイル名を下記に変更 key.json ・解凍したZipファイルのcredentialに配置 ～～～mirameetVol24-main\\src\\credential "},"Section4.html":{"url":"Section4.html","title":"3. 各処理の実行と処理結果の確認","keywords":"","body":"各処理の実行と処理結果の確認 この手順では、手順２で修正したOperationObject.pyを含む各ソースを動かし、BigQueryのデータを操作していきます。 カレントディレクトリの移動 Zipファイルを解凍したディレクトリまで移動 cd ～～～mirameetVol24-main Dockerでコンテナを起動 docker-compose up -d --build 起動したDockerコンテナに接続 docker-compose exec app bash GcsUploader01.pyの実行 CSVデータファイルをGCSバケット上にアップロードする python GcsUploader01.py ▼中身 import os import OperationObject # 操作対象の設定情報取得 from google.cloud import storage # GCP認証設定 os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = OperationObject.GOOGLE_APPLICATION_CREDENTIALS # GCSクライアントAPIの利用宣言 client = storage.Client() # GCSバケット取得 bucket = client.get_bucket(OperationObject.bucket_name) # CSVファイルアップロード blob = bucket.blob(os.path.basename(OperationObject.source_file_name)) blob.upload_from_filename(OperationObject.source_file_name) print('File {} uploaded to {}.'.format( OperationObject.source_file_name, bucket)) ↓正常終了mirameetVol24-main\\src\\csvの中の「gcs-example.csv」が、GCSにアップロードされていることを確認 GcsToBigQuery02.pyの実行 GCSにアップロードしたCSVデータをBigQueryにインサートする python GcsToBigQuery02.py ▼中身 import os import OperationObject # 操作対象の設定情報取得 from google.cloud import bigquery # GCP認証設定 os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = OperationObject.GOOGLE_APPLICATION_CREDENTIALS # BigQueryクライアントAPIを利用宣言 client = bigquery.Client(OperationObject.project_id) # テーブル情報の取得 table_info = client.dataset(OperationObject.dataset_id).table(OperationObject.table_id) # 登録対象のテーブル定義を設定 job_config = bigquery.LoadJobConfig( # テーブルカラムマッピング情報の設定 schema=[ bigquery.SchemaField(\"id\", \"NUMERIC\"), bigquery.SchemaField(\"mira_code\", \"STRING\"), bigquery.SchemaField(\"mira_text\", \"STRING\"), bigquery.SchemaField(\"work_date\", \"STRING\") ], # 読み込み開始行の指定（ヘッダ行がないため0を設定） skip_leading_rows=0, # ソースフォーマットの指定（CSV形式に設定） source_format=bigquery.SourceFormat.CSV, ) # GCSバケットをロードし、テーブルに登録 load_job = client.load_table_from_uri( OperationObject.url_gs_example_csv, table_info, job_config=job_config ) load_job.result() # load_table_from_uriが終了するまで待機 regist_table_info = client.get_table(table_info) # 登録後のテーブル情報取得 print(\"Loaded {} rows.\".format(regist_table_info.num_rows)) ↓正常終了「gcs-example.csv」の中のデータがBigQueryにインサートされていることを確認プレビュータグでデータの中身を確認 UpdateDeleteBigQuery03.pyの実行 BigQueryのデータを更新・削除する python UpdateDeleteBigQuery03.py ▼中身 import os import OperationObject # 操作対象の設定情報取得 from google.cloud import bigquery # GCP認証設定 os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = OperationObject.GOOGLE_APPLICATION_CREDENTIALS # BigQueryクライアントAPIの利用宣言 client = bigquery.Client() # 更新SQL生成 updateQuery = \"UPDATE `{0}.{1}.{2}` SET mira_text = '更新' WHERE id = 2\".\\ format(OperationObject.project_id, OperationObject.dataset_id, OperationObject.table_id) # SQL実行 client.query(updateQuery).result() print(\"Updated ID=2.\") # 削除SQL生成 deleteQuery = \"DELETE `{0}.{1}.{2}` WHERE id = 3\".\\ format(OperationObject.project_id, OperationObject.dataset_id, OperationObject.table_id) # SQL実行 client.query(deleteQuery).result() print(\"Deleted ID=3.\") ↓正常終了ID＝2のデータが更新、ID＝3のデータが削除されていることを確認 ExportBigQuery04.pyの実行 BigQueryのデータをCSVデータファイルとしてGCSバケットにエクスポートする python ExportBigQuery04.py ▼中身 import os import OperationObject # 操作対象の設定情報取得 from google.cloud import bigquery # GCP認証設定 os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = OperationObject.GOOGLE_APPLICATION_CREDENTIALS # BigQueryクライアントAPIの利用宣言 client = bigquery.Client(OperationObject.project_id) # テーブル情報の取得 table_info = client.dataset(OperationObject.dataset_id).table(OperationObject.table_id) # データ取得結果をGCSバケットにエキスポート extract_job = client.extract_table( table_info, OperationObject.out_url_gs_example_csv, ) extract_job.result() # extract_tableが終了するまで待機 print( \"Exported {}:{}.{} to {}\".format( OperationObject.project_id, OperationObject.dataset_id, OperationObject.table_id, OperationObject.out_url_gs_example_csv) ) ↓正常終了BigQueryのデータが「out-gcs-example.csv」ファイルとしてGCSに出力されていることを確認ダウンロードボタンでファイルをローカルにダウンロードダウンロードしたCSVファイルの中身で更新後の状態を確認 "},"Section5.html":{"url":"Section5.html","title":"4. おまけ","keywords":"","body":"おまけ この手順では、作成したテーブルやバケットの削除をします ※無料期間が終了しても自動で課金されることはありません　GCP内をCleanUpしたい方は以下手順を行ってください バケットの削除 データセットの削除 Dockerコンテナの停止 起動中のコンテナの確認 docker-compose ps コンテナの停止 docker-compose down コンテナが停止されたことの確認 docker-compose ps "}}